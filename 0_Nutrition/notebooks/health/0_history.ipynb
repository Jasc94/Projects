{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ml_model_plotter():\n",
    "    '''\n",
    "    Class to plot machine learning model results\n",
    "    '''\n",
    "    #####\n",
    "    @staticmethod\n",
    "    def train_val_plot(ml_model, figsize = (14, 6)):\n",
    "        '''\n",
    "        It plots training scores vs validation scores. It returns a figure\n",
    "        '''\n",
    "        fig = plt.figure(figsize = figsize)\n",
    "        sns.set_theme()\n",
    "\n",
    "        sns.lineplot(data = [ml_model.train_scores, ml_model.val_scores], markers = True, dashes = False)\n",
    "\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.xlabel(\"Round\")\n",
    "        plt.legend([\"Train score\", \"Validation score\"])\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    #####\n",
    "    @staticmethod\n",
    "    def test_metrics(ml_model, figsize = (12, 12)):\n",
    "        '''\n",
    "        It plots the metrics after training with the full training data and testing with the test data. It returns a figure\n",
    "        '''\n",
    "        # Calculate the row/column totals for later use\n",
    "        row_sums = ml_model.cm.sum(axis = 1, keepdims = True)\n",
    "        column_sums = ml_model.cm.sum(axis = 0, keepdims = True)\n",
    "        \n",
    "        # Relative values to column/row sums\n",
    "        rel_row = (ml_model.cm / row_sums) * 100\n",
    "        rel_col = (ml_model.cm / column_sums) * 100\n",
    "\n",
    "        # Plot\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize = figsize, sharex = True, sharey = True)\n",
    "\n",
    "        first_row_palette = sns.color_palette(\"light:b\", as_cmap=True)\n",
    "        second_row_palette = sns.light_palette(\"seagreen\", as_cmap=True)\n",
    "        fmt = \"g\"\n",
    "\n",
    "        # ax1\n",
    "        sns.heatmap(ml_model.cm, annot = True, linewidths = .1, cmap = first_row_palette, ax = ax1, cbar = False, fmt = fmt)\n",
    "        ax1.set_ylabel(\"Actual class\")\n",
    "        ax1.set_title(\"Confusion matrix\")\n",
    "\n",
    "        # ax2\n",
    "        sns.heatmap((ml_model.cm / ml_model.cm.sum()) * 100, annot = True, linewidths = .1, cmap = first_row_palette, ax = ax2, cbar = False, fmt = fmt)\n",
    "        ax2.set_ylabel(\"Actual class\")\n",
    "        ax2.set_title(\"Confusion matrix - relative\")\n",
    "\n",
    "        # ax3\n",
    "        sns.heatmap(rel_row, annot = True, linewidths = .1, cmap = second_row_palette, ax = ax3, cbar = False, fmt = fmt)\n",
    "        ax3.set_xlabel(\"Predicted class\")\n",
    "        ax3.set_title(\"Relative to row sum (Recall)\")\n",
    "\n",
    "        # ax4\n",
    "        sns.heatmap(rel_col, annot = True, linewidths = .1, cmap = second_row_palette, ax = ax4, cbar = False, fmt = fmt)\n",
    "        ax4.set_xlabel(\"Predicted class\")\n",
    "        ax4.set_title(\"Relative to col sum (Precision)\")\n",
    "\n",
    "        return fig"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class eda_plotter():\n",
    "    #####\n",
    "    @staticmethod\n",
    "    def __n_rows(df, n_columns):\n",
    "        '''\n",
    "        It calculates the number of rows (for the axes) depending on the number of variables to plot and the columns we want for the figure.\n",
    "        args:\n",
    "        n_columns: number of columns\n",
    "        '''\n",
    "        columns = list(df.columns)\n",
    "\n",
    "        if len(columns) % n_columns == 0:\n",
    "            axes_rows = len(columns) // n_columns\n",
    "        else:\n",
    "            axes_rows = (len(columns) // n_columns) + 1\n",
    "\n",
    "        return axes_rows\n",
    "\n",
    "    #####\n",
    "    @staticmethod\n",
    "    def rows_plotter(df, features_names, n_columns, kind = \"box\", figsize = (12, 6)):\n",
    "        '''\n",
    "        It plots all the variables in one row. It returns a figure\n",
    "        args:\n",
    "        n_columns: number of columns for the row\n",
    "        kind: (\"strip\", \"dist\", \"box\")\n",
    "        figsize: size of the figure\n",
    "        '''\n",
    "        # creates a figure with one axis and n_columns\n",
    "        fig, axes = plt.subplots(1, n_columns, figsize = figsize)\n",
    "        count = 0\n",
    "\n",
    "        # Loop thorugh the generated axes\n",
    "        for column in range(n_columns):\n",
    "            if kind == \"strip\":\n",
    "                sns.stripplot(y = df.iloc[:, count], ax = axes[column])\n",
    "            elif kind == \"dist\":\n",
    "                sns.distplot(df.iloc[:, count], ax = axes[column])\n",
    "            elif kind == \"box\":\n",
    "                sns.boxplot(df.iloc[:, count], ax = axes[column])\n",
    "            else:\n",
    "                sns.histplot(df.iloc[:, count], ax = axes[column], bins = 30)\n",
    "\n",
    "            try:\n",
    "                axes[column].set(xlabel = features_names[count])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if (count + 1) < df.shape[1]:\n",
    "                    count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return fig\n",
    "\n",
    "    #####\n",
    "    @staticmethod\n",
    "    def multi_axes_plotter(df, features_names, n_columns, kind = \"box\", figsize = (12, 12)):\n",
    "        '''\n",
    "        It creates a plot with multiple rows and columns. It returns a figure.\n",
    "        n_columns: number of columns for the row\n",
    "        kind: (\"strip\", \"dist\", \"box\")\n",
    "        figsize: size of the figure\n",
    "        '''\n",
    "        # Calculating the number of rows from number of columns and variables to plot\n",
    "        n_rows_ = eda_plotter.__n_rows(df, n_columns)\n",
    "\n",
    "        # Creating the figure and as many axes as needed\n",
    "        fig, axes = plt.subplots(n_rows_, n_columns, figsize = figsize)\n",
    "        # To keep the count of the plotted variables\n",
    "        count = 0\n",
    "\n",
    "        # Some transformation, because with only one row, the shape is: (2,)\n",
    "        axes_col = axes.shape[0]\n",
    "        try:\n",
    "            axes_row = axes.shape[1]\n",
    "        except:\n",
    "            axes_row = 1\n",
    "\n",
    "        # Loop through rows\n",
    "        for row in range(axes_col):\n",
    "            # Loop through columns\n",
    "            for column in range(axes_row):\n",
    "                if kind == \"strip\":\n",
    "                    sns.stripplot(y = df.iloc[:, count], ax = axes[row][column])\n",
    "                elif kind == \"dist\":\n",
    "                    sns.distplot(df.iloc[:, count], ax = axes[row][column])\n",
    "                elif kind == \"box\":\n",
    "                    sns.boxplot(df.iloc[:, count], ax = axes[row][column])\n",
    "                else:\n",
    "                    sns.histplot(df.iloc[:, count], ax = axes[row][column], bins = 30)\n",
    "\n",
    "                try:\n",
    "                    axes[row][column].set(xlabel = features_names[count])\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                if (count + 1) < df.shape[1]:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    break\n",
    "        return fig\n",
    "\n",
    "    #####\n",
    "    @staticmethod\n",
    "    def correlation_matrix(df, features_names, figsize = (12, 12)):\n",
    "        '''\n",
    "        It plots a correlation matrix. It returns a figure\n",
    "        '''\n",
    "        fig = plt.figure(figsize = figsize)\n",
    "        sns.heatmap(df.corr(), annot = True, linewidths = .1,\n",
    "                    cmap = \"Blues\", xticklabels = False,\n",
    "                    yticklabels = features_names, cbar = False)\n",
    "\n",
    "        return fig"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from varname import nameof\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html\n",
    "import lxml\n",
    "\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import sys, os\n",
    "\n",
    "class dataset:\n",
    "    '''\n",
    "    Object that will hold information about dataframe as well as do some useful transformations and save a copy in case we need to go back to the unprocessed version of the dataframe\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Raw data\n",
    "        self.__dfs_list = []\n",
    "        self.__joined_dfs = {}\n",
    "        self.__raw_df = None\n",
    "        self.df = None\n",
    "\n",
    "        # Processed data for ML\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.kfold = None\n",
    "\n",
    "    ######### DATA PROCESSING #########\n",
    "    #########\n",
    "    def __read_data(self, data_path):\n",
    "        '''\n",
    "        It reads all the files from a folder as dataframes, and saves them all in a dict with the name of the file as a key.\n",
    "        args:\n",
    "        up_levels: steps to go up from current folder\n",
    "        folder: where the files are located\n",
    "        '''\n",
    "        data_dfs = {}\n",
    "        for file_ in os.listdir(data_path):\n",
    "            if file_ != \"history\":\n",
    "                try:\n",
    "                    # Path to file\n",
    "                    filepath = data_path + sep + file_\n",
    "\n",
    "                    # Reading as dataframe\n",
    "                    df = pd.read_csv(filepath, index_col = 0)\n",
    "                    df[\"SEQN\"] = df[\"SEQN\"].map(int)\n",
    "                    df.set_index(\"SEQN\", inplace = True)\n",
    "\n",
    "                    # Saving it in a dictionary\n",
    "                    dict_key = file_[:-4].lower()\n",
    "                    data_dfs[dict_key] = df\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        return data_dfs\n",
    "\n",
    "    #########\n",
    "    def __read_all_data(self, data_path, folders):\n",
    "        '''\n",
    "        It does the same as __read_data but for several folders at the same time\n",
    "        args: same as __read_data\n",
    "        '''\n",
    "        for folder in folders:\n",
    "            folder_path = data_path + folder\n",
    "            self.__dfs_list.append(self.__read_data(folder_path))\n",
    "\n",
    "    #########\n",
    "    def __concatenate_dfs(self, data_dfs):\n",
    "        '''\n",
    "        It receives a dict of dataframes and combines them by name\n",
    "        args:\n",
    "        data_dfs: dict with filename as key and dataframe as value\n",
    "        '''\n",
    "        files = {}\n",
    "        count = 0\n",
    "\n",
    "        for key, dfs in data_dfs.items():\n",
    "            key_ = key[:-2]\n",
    "\n",
    "            if count == 0:\n",
    "                files[key_] = dfs\n",
    "            else:\n",
    "                if key_ not in files.keys():\n",
    "                    files[key_] = dfs\n",
    "                else:\n",
    "                    files[key_] = pd.concat([files[key_], dfs])\n",
    "\n",
    "            count +=1\n",
    "\n",
    "        return files\n",
    "\n",
    "    #########\n",
    "    def __concatenate_all_dfs(self):\n",
    "        '''\n",
    "        It does the same as __concatenate_dfs but for multiple dicts\n",
    "        '''\n",
    "        for data_dfs in self.__dfs_list:\n",
    "            files = self.__concatenate_dfs(data_dfs)\n",
    "            self.__joined_dfs = {**self.__joined_dfs, **files}\n",
    "\n",
    "\n",
    "    #########\n",
    "    def __merge_dfs(self):\n",
    "        '''\n",
    "        It combines all dfs processed into one\n",
    "        '''\n",
    "        keys = list(self.__joined_dfs.keys())\n",
    "        self.df = self.__joined_dfs.pop(keys[0])\n",
    "\n",
    "        for name, df in self.__joined_dfs.items():\n",
    "            self.df = pd.merge(self.df, df, how = \"outer\", on = \"SEQN\")\n",
    "            \n",
    "    #########\n",
    "    def __clean_rows(self):\n",
    "        '''\n",
    "        It removes values (rows) of no interest for specific columns. Values such as 7 or 9 that represent either \"No answer\" or \"No info\"\n",
    "        '''\n",
    "        important_values = [7.0, 9.0]\n",
    "        # Asthma\n",
    "        self.df = self.df[~self.df.MCQ010.isin(important_values)]\n",
    "        # Heart problems\n",
    "        self.df = self.df[~self.df.MCQ160B.isin(important_values)]\n",
    "        self.df = self.df[~self.df.MCQ160C.isin(important_values)]\n",
    "        self.df = self.df[~self.df.MCQ160D.isin(important_values)]\n",
    "        self.df = self.df[~self.df.MCQ160E.isin(important_values)]\n",
    "        self.df = self.df[~self.df.MCQ160F.isin(important_values)]\n",
    "\n",
    "    def __update_target_values(self):\n",
    "        '''\n",
    "        It replaces the 2s with 0s for potential target variables\n",
    "        '''\n",
    "        self.df.MCQ010 = self.df.MCQ010.replace(2, 0)\n",
    "        self.df.MCQ160B = self.df.MCQ160B.replace(2, 0)\n",
    "        self.df.MCQ160C = self.df.MCQ160C.replace(2, 0)\n",
    "        self.df.MCQ160D = self.df.MCQ160D.replace(2, 0)\n",
    "        self.df.MCQ160E = self.df.MCQ160E.replace(2, 0)\n",
    "        self.df.MCQ160F = self.df.MCQ160F.replace(2, 0)\n",
    "\n",
    "    #########\n",
    "    def __clean_columns(self, correction_map):\n",
    "        '''\n",
    "        It removes duplicated columns.\n",
    "        args:\n",
    "        correction_map: dict which keys are the columns to rename and the values are the new names for those columns\n",
    "        '''\n",
    "        to_drop = [key[:-2] + \"_y\" for key in correction_map.keys()]\n",
    "        self.df = self.df.drop(to_drop, axis = 1)\n",
    "        self.df = self.df.rename(columns = correction_map)\n",
    "\n",
    "    #########\n",
    "    def __heart_disease(self):\n",
    "        '''\n",
    "        It creates a new column using all cardiovascular-related ones as source. The objective is to have a new column where we can see if the participant has any kind of heart disease.\n",
    "        '''\n",
    "        # We create the column and fill it in with NaN values, as the initial status (with no information) is that we don't know whether someone has o doesn't have a coronary disease\n",
    "        self.df[\"MCQ160H\"] = np.nan\n",
    "\n",
    "        # Conditions to filter by any heart disease\n",
    "        pos_cond_b = self.df.MCQ160B == 1\n",
    "        pos_cond_c = self.df.MCQ160C == 1\n",
    "        pos_cond_d = self.df.MCQ160D == 1\n",
    "        pos_cond_e = self.df.MCQ160E == 1\n",
    "        pos_cond_f = self.df.MCQ160F == 1\n",
    "\n",
    "        # For those participants we do have the info for and we know they don't have any coronary disease\n",
    "        neg_cond_b = self.df.MCQ160B == 0\n",
    "        neg_cond_c = self.df.MCQ160C == 0\n",
    "        neg_cond_d = self.df.MCQ160D == 0\n",
    "        neg_cond_e = self.df.MCQ160E == 0\n",
    "        neg_cond_f = self.df.MCQ160F == 0\n",
    "\n",
    "        # Given the positive conditions, place a \"1\" in the column if they are matched\n",
    "        self.df.loc[(pos_cond_b) | (pos_cond_c) | (pos_cond_d) | (pos_cond_e) | (pos_cond_f), \"MCQ160H\"] = 1\n",
    "        # Given the negative conditions, place a \"0\" in the column if they are matched\n",
    "        self.df.loc[(neg_cond_b) & (neg_cond_c) & (neg_cond_d) & (neg_cond_e) & (neg_cond_f), \"MCQ160H\"] = 0\n",
    "\n",
    "    #########\n",
    "    def load_data(self, data_path, folders, correction_map):\n",
    "        '''\n",
    "        It combines all previous steps to get clean and ready-to-use data\n",
    "        '''\n",
    "        self.__read_all_data(data_path, folders)\n",
    "        self.__concatenate_all_dfs()\n",
    "        self.__merge_dfs()\n",
    "        self.__clean_rows()\n",
    "        self.__update_target_values()\n",
    "        self.__clean_columns(correction_map)\n",
    "        self.__heart_disease()\n",
    "        # Dataset backup\n",
    "        self.__raw_df = self.df\n",
    "    \n",
    "    ######### SUPPORT FUNCTIONS #########\n",
    "    #########\n",
    "    def filter_columns(self, features, inplace = False):\n",
    "        '''\n",
    "        It filters the dataframe.\n",
    "        args:\n",
    "        features: columns we want to filter by\n",
    "        inplace: default = False. If True, it will modify the dataframe within the object.\n",
    "        '''\n",
    "        if inplace:\n",
    "            self.df = self.df.loc[:, features]\n",
    "        else:\n",
    "            return self.df.loc[:, features]\n",
    "\n",
    "    #########\n",
    "    def drop_columns(self, columns):\n",
    "        '''\n",
    "        To drop columns\n",
    "        '''\n",
    "        self.df = self.df.drop(columns, axis = 1)\n",
    "\n",
    "    #########\n",
    "    def drop_nans(self):\n",
    "        '''\n",
    "        To drop nans\n",
    "        '''\n",
    "        self.df = self.df.dropna()\n",
    "\n",
    "    #########\n",
    "    def dummies_transform(self, variable, mapper):\n",
    "        '''\n",
    "        Transforms categorical variables into dummies.\n",
    "        args:\n",
    "        variable: target column to be transformed\n",
    "        mapper: To preprocess the values before transforming the column into dummies.\n",
    "        '''\n",
    "        # Mapping values\n",
    "        self.df.loc[:, variable] = self.df.loc[:, variable].map(mapper)\n",
    "        # Getting dummies\n",
    "        self.df = pd.get_dummies(self.df, prefix = \"\", prefix_sep = \"\", columns = [variable])\n",
    "        #return df\n",
    "\n",
    "    #########\n",
    "    def __pair_mean(self, pair_list, new_name, drop_old = False):\n",
    "        '''\n",
    "        It creates a new column by calculating the mean of two other.\n",
    "        args:\n",
    "        pair_list: columns to calculate the mean of\n",
    "        new_name: name for the new column\n",
    "        drop_old: set to False by default. If True, it will remove the columns we used to calculated the mean of\n",
    "        '''\n",
    "        self.df[new_name] = self.df.loc[:, pair_list].mean(axis = 1)\n",
    "        \n",
    "        if drop_old:\n",
    "            self.df = self.df.drop(pair_list, axis = 1)\n",
    "\n",
    "    #########\n",
    "    def pairs_mean(self, combination_list, drop_old = False):\n",
    "        '''\n",
    "        It does the same as __pair_mean but for several pairs at once.\n",
    "        args:\n",
    "        combination_list: [[var1, var2], new_var]\n",
    "        drop_old: By default set to False. If True, it will remove the variables used to calculated the mean.\n",
    "        '''\n",
    "        for combination in combination_list:\n",
    "            self.__pair_mean(combination[0], combination[1], drop_old = drop_old)\n",
    "\n",
    "    #########\n",
    "    def reset_dataset(self):\n",
    "        '''\n",
    "        In case we want to restore the dataset to its first status (when used load_data method)\n",
    "        '''\n",
    "        self.df = self.__raw_df\n",
    "\n",
    "    #########\n",
    "    def model_data(self, split, cv, epochs = 1, scaler = False, balance = None, seed = 42): \n",
    "        '''\n",
    "        It allows us to prepare the data for Machine Learning training\n",
    "        '''\n",
    "        # Independent variables\n",
    "        X = np.array(self.df.iloc[:, 1:])\n",
    "\n",
    "        # Dependent variable\n",
    "        y = np.array(self.df.iloc[:, 0])\n",
    "\n",
    "        # Data scaling\n",
    "        if scaler:\n",
    "            scaler = StandardScaler()\n",
    "            X = scaler.fit_transform(X)\n",
    "\n",
    "        # Train-test\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size = split, random_state = seed)\n",
    "\n",
    "        # Balancing data\n",
    "        if balance != None:\n",
    "            sm = SMOTE(sampling_strategy = balance, random_state = seed, n_jobs = -1)\n",
    "            self.X_train, self.y_train = sm.fit_resample(self.X_train, self.y_train)\n",
    "\n",
    "        # Cross validation\n",
    "        self.kfold = RepeatedStratifiedKFold(n_splits = cv, n_repeats = epochs, random_state = seed)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}